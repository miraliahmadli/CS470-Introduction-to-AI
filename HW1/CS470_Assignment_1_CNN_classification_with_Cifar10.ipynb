{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CS470 Assignment #1: CNN classification with Cifar10",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqPMAzzNipD",
        "colab_type": "text"
      },
      "source": [
        "CS470 Assignment #1: CNN classification with Cifar10\n",
        "====\n",
        "\n",
        "Primary TA : Myeongjae Jang\n",
        "\n",
        "TA's E-mail : myeongjae0409@kaist.ac.kr\n",
        "\n",
        "## Instruction\n",
        "\n",
        "- Modify the baseline CNN model to improve the classification performance on Cifar10 dataset. In addition to the model definition, you can modify any parts of this colab example to improve the test accuracy (e.g., learning rate, batch size, etc.)\n",
        "- Train your CNN model and compare it to the baseline (in terms of training loss and the test accuracy).\n",
        "- Explain your modifications and discuss how you improved the test accuracy.\n",
        "\n",
        "## Submission guidelines\n",
        "\n",
        "- Your code and report will be all in Colab. Copy this example to your google drive and edit it to complete your assignment. Add sections at the bottom of this example to discuss the results. For discussion and analysis, we highly encourage you to use graphics if possible (e.g., plots, images, etc.). \n",
        "- To make grading efficient, please highlight all contributions & modifications you made clearly. We highly encourage you to add code blocks in the discussion section to discuss your modifications (e.g., you can describe the model definition in the discussion section using the code blocks).\n",
        "- We should be able to reproduce your results using your code and pre-trained model. Please double-check if your code runs without error and loads your pre-trained model properly. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
        "- In this assignment, **we are not allowing fine-tuning from the pre-trained model** (e.g. ImageNet pre-trained models). You should train your  model on Cifar10 dataset from scratch. \n",
        "\n",
        "## Deliverables\n",
        "- Download your Colab notebook and the pre-trained model, and submit a zip file in a format: [StudentID].zip. Please double-check that you locate and load your pre-trained model properly.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions. \n",
        "\n",
        "## Grading policy\n",
        "\n",
        "- **Code** (50%): Your code should work and outperform the baseline model in terms of the test accuracy. \n",
        "- **Report** (50%): Explain your modification and justify how it improved the perofrmance. It would be great if you have some supporting results for your justification (e.g., justifying that you resolved the overfitting by comparing two training/testing loss curves). \n",
        "- **Extra points** will be given if your submission satisfies the following:\n",
        " - **High test accuracy**: we will rank the submissions based on the test accuracy, and assign extra points according to the rank (e.g. 3 points for top 10%, 2 points for top 30%, 1 points for top 50%.)\n",
        " - **Comprehensive discussion**: we will assign extra points if your report contains comprehensive discussion/analysis of the results. Examples include justification of your choice of model (or hyper-parameters), comparisons to the baseline model (analysis on the source of improvement), insightful visualizations (loss curves, misclassification results), etc.\n",
        "\n",
        "## Due date\n",
        "- **23:59:59 September 25th.**\n",
        "- Late submission is allowed until 23:59:59 September 27th.\n",
        "- Late submission will be applied 20% penalty.\n",
        "\n",
        "## Questions\n",
        "- Please use QnA board in KLMS as a main communication channel. When you post questions, please make it public so that all students can share the information. Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n",
        "\n",
        "## PyTorch Documentation\n",
        "- You can refer PyTorch documentation for your assignment.\n",
        "- https://pytorch.org/docs/stable/index.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1mgGV_uOIK",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Connect to your Google Drive\n",
        "\n",
        "It is required if you want to save checkpoints and load them later on.\n",
        "\n",
        "### (You have to submit your trained results as the checkpoint. So, please check your Google Drive connection again.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLth6ZfXuSGT",
        "colab_type": "code",
        "outputId": "322bad93-254f-4c2f-8750-40c72e2134f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwUwGf8qW1U",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtshANjqpy4",
        "colab_type": "code",
        "outputId": "56e72d08-7109-4a67-e563-0140d567f56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJ-Q6sbq8c3",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Configure the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5jAy7Wq-E2",
        "colab_type": "code",
        "outputId": "33897269-59de-47c5-d818-07c9b6bd95b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 450\n",
        "learning_rate = 0.1\n",
        "batch_size = 128\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://6d53fc21.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tZt60aMrQ1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Construct data pipeline\n",
        "\n",
        "**`torchvision.datasets.CIFAR10`** will automatically construct **`Cifar10`** dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbtV46LrXOF",
        "colab_type": "code",
        "outputId": "2e6edc0d-0595-4f29-ac3a-9691cc4682bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_dir = os.path.join(gdrive_root, 'my_data')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_dWd-6rwWb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Construct a neural network builder\n",
        "\n",
        "We serve the baseline CNN model which is supported on Pytorch tutorial: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb#scrollTo=c1E1b7-igUcR\n",
        "\n",
        "### (You have to compare your own CNN model's test accuracy with the baseline CNN model and explain why your own model's test accuracy is higher than the basline.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_wne0Vr1E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MyClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features=6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 1 * 1, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(self.relu(self.batchnorm1(self.conv1(x))))\n",
        "      x = self.pool(self.relu(self.conv2(x)))\n",
        "      x = self.pool(self.relu(self.batchnorm3(self.conv3(x))))\n",
        "      x = self.pool(self.relu(self.conv4(x)))\n",
        "      x = x.view(-1, 64 * 1 * 1)\n",
        "      x = self.relu(self.fc1(x))\n",
        "      outputs = self.fc2(x)\n",
        "      return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdcevVXo9piX",
        "colab_type": "text"
      },
      "source": [
        "# VGG19 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WPolkuv9ppo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers([64, 64, 'Pool', 128, 128, 'Pool', 256, 256, 256, 256, 'Pool', 512, 512, 512, 512, 'Pool', 512, 512, 512, 512, 'Pool'])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'Pool':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpA3xhjMspvA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Initialize the network and optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP111gW0s8aH",
        "colab_type": "code",
        "outputId": "1fd89cf5-53c6-4b8a-8480-e5362e11193b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#my_classifier = MyClassifier()\n",
        "my_classifier = VGG()\n",
        "my_classifier = my_classifier.to(device)\n",
        "if device == 'cuda':\n",
        "    my_classifier = torch.nn.DataParallel(my_classifier)\n",
        "    cudnn.benchmark = True\n",
        "# Print your neural network structure\n",
        "print(my_classifier)\n",
        "\n",
        "#optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n",
        "optimizer = optim.SGD(my_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataParallel(\n",
            "  (module): VGG(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace)\n",
            "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace)\n",
            "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (9): ReLU(inplace)\n",
            "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (12): ReLU(inplace)\n",
            "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (16): ReLU(inplace)\n",
            "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (19): ReLU(inplace)\n",
            "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (22): ReLU(inplace)\n",
            "      (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (25): ReLU(inplace)\n",
            "      (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (29): ReLU(inplace)\n",
            "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (32): ReLU(inplace)\n",
            "      (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (35): ReLU(inplace)\n",
            "      (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (38): ReLU(inplace)\n",
            "      (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (42): ReLU(inplace)\n",
            "      (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (45): ReLU(inplace)\n",
            "      (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (48): ReLU(inplace)\n",
            "      (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (51): ReLU(inplace)\n",
            "      (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (53): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "    )\n",
            "    (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmCT7kCU-SGp",
        "colab_type": "text"
      },
      "source": [
        "# Schedule learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqSBdSF6-SRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR \n",
        "\n",
        "scheduler = MultiStepLR(optimizer, milestones=[150,300], gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAQeXmjsILS",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Load pre-trained weights if exist\n",
        "\n",
        "- **For your sumbmission you have to store the trained model as a checkpoint.**\n",
        "- Please do not erase this step.\n",
        "- If you want to modify this step, please be careful.\n",
        "- After training please confirm that your checkpoint is correctly stored and re-loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLNZxaBsHUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest_vgg_final.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1t7n6yttNEc",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train the network\n",
        "\n",
        "Note : It would be better to save checkpoints periodically, otherwise you'll lose everything you've trained if the session is recycled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vczdKbytV38",
        "colab_type": "code",
        "outputId": "f413c6a1-afb6-4904-c68b-d80a44d5ae44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "if training_process:\n",
        "  it = 0\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(max_epoch):\n",
        "    scheduler.step()\n",
        "    # train phase\n",
        "    my_classifier.train()\n",
        "    for inputs, labels in train_dataloader:\n",
        "      it += 1\n",
        "\n",
        "      # load data to the GPU.\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # feed data into the network and get outputs.\n",
        "      logits = my_classifier(inputs)\n",
        "\n",
        "      # calculate loss\n",
        "      # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
        "      #loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "      #Since I use VGG model and there is Linear Layer in the end, using nn.CrossEntropyLoss is more convenient\n",
        "      loss = cross_entropy(logits, labels)\n",
        "\n",
        "      # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "      #       Otherwise, gradients will accumulate.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backprogate loss.\n",
        "      loss.backward()\n",
        "\n",
        "      # update the weights in the network.\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate accuracy.\n",
        "      acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "      if it % 250 == 0:\n",
        "        tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
        "        print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
        "\n",
        "    # save losses in a list so that we can visualize them later.\n",
        "    train_losses.append(loss)  \n",
        "\n",
        "    # test phase\n",
        "    n = 0.\n",
        "    test_loss = 0.\n",
        "    test_acc = 0.\n",
        "    my_classifier.eval()\n",
        "    for test_inputs, test_labels in test_dataloader:\n",
        "      test_inputs = test_inputs.to(device)\n",
        "      test_labels = test_labels.to(device)\n",
        "\n",
        "      logits = my_classifier(test_inputs)\n",
        "      #loss = cross_entropy(logits, test_labels, reduction='sum')\n",
        "      test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "      #loss = nn.CrossEntropyLoss(logits, test_labels, False)\n",
        "      #test_loss += loss.item()\n",
        "      test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "      n += test_inputs.size(0)\n",
        "\n",
        "    test_loss /= n\n",
        "    test_acc /= n\n",
        "    test_losses.append(test_loss)\n",
        "    tbc.save_value('Loss', 'test_loss', it, test_loss)\n",
        "    print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss, test_acc)) \n",
        "\n",
        "    tbc.flush_line('train_loss')\n",
        "    tbc.flush_line('test_loss')\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'my_classifier':my_classifier.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_acc':best_acc}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    \n",
        "tbc.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:101: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "[epoch:0, iteration:250] train loss : 1.9856 train accuracy : 0.2031\n",
            "[epoch:0, iteration:391] test_loss : 1.8244 test accuracy : 0.2631\n",
            "checkpoint is saved !\n",
            "[epoch:1, iteration:500] train loss : 1.7669 train accuracy : 0.2578\n",
            "[epoch:1, iteration:750] train loss : 1.4269 train accuracy : 0.4297\n",
            "[epoch:1, iteration:782] test_loss : 1.4724 test accuracy : 0.4369\n",
            "checkpoint is saved !\n",
            "[epoch:2, iteration:1000] train loss : 1.3464 train accuracy : 0.5391\n",
            "[epoch:2, iteration:1173] test_loss : 1.2618 test accuracy : 0.5516\n",
            "checkpoint is saved !\n",
            "[epoch:3, iteration:1250] train loss : 0.9883 train accuracy : 0.6562\n",
            "[epoch:3, iteration:1500] train loss : 0.9842 train accuracy : 0.6562\n",
            "[epoch:3, iteration:1564] test_loss : 0.9819 test accuracy : 0.6652\n",
            "checkpoint is saved !\n",
            "[epoch:4, iteration:1750] train loss : 0.8734 train accuracy : 0.6328\n",
            "[epoch:4, iteration:1955] test_loss : 1.0783 test accuracy : 0.6377\n",
            "[epoch:5, iteration:2000] train loss : 0.8124 train accuracy : 0.7656\n",
            "[epoch:5, iteration:2250] train loss : 0.8697 train accuracy : 0.6953\n",
            "[epoch:5, iteration:2346] test_loss : 0.7681 test accuracy : 0.7452\n",
            "checkpoint is saved !\n",
            "[epoch:6, iteration:2500] train loss : 0.5563 train accuracy : 0.8359\n",
            "[epoch:6, iteration:2737] test_loss : 1.2605 test accuracy : 0.6236\n",
            "[epoch:7, iteration:2750] train loss : 0.6248 train accuracy : 0.7891\n",
            "[epoch:7, iteration:3000] train loss : 0.6112 train accuracy : 0.7656\n",
            "[epoch:7, iteration:3128] test_loss : 1.0048 test accuracy : 0.6840\n",
            "[epoch:8, iteration:3250] train loss : 0.4150 train accuracy : 0.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECu3yS0OvfoR",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Visualize and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sqVp-vLRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "\n",
        "if not training_process:\n",
        "  # Re-load trained model\n",
        "  my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "  optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "  # Testing\n",
        "  n = 0.\n",
        "  test_loss = 0.\n",
        "  test_acc = 0.\n",
        "  my_classifier.eval()\n",
        "  for test_inputs, test_labels in test_dataloader:\n",
        "    test_inputs = test_inputs.to(device)\n",
        "    test_labels = test_labels.to(device)\n",
        "\n",
        "    logits = my_classifier(test_inputs)\n",
        "    test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "    test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "    n += test_inputs.size(0)\n",
        "\n",
        "  test_loss /= n\n",
        "  test_acc /= n\n",
        "  print('Test_loss : {:.4f}, Test accuracy : {:.4f}'.format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjD9SGrvhzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "my_classifier.eval()\n",
        "\n",
        "num_test_samples = len(test_dataset)\n",
        "random_idx = random.randint(0, num_test_samples)\n",
        "\n",
        "test_input, test_label = test_dataset.__getitem__(random_idx)\n",
        "test_prediction = F.softmax(my_classifier(test_input.unsqueeze(0).to(device)), dim=1).argmax().item()\n",
        "print('label : %s' % classes[test_label])\n",
        "print('prediction : %s' % classes[test_prediction])\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(test_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwcY8z_C2QLA",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2W0ZtF82Xyb",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\"> \n",
        "Baseline model\n",
        "---\n",
        "\n",
        "</font>\n",
        "<pre>\n",
        "**Model Summary**\n",
        "----------------------------------------------------------------  \n",
        "        Layer (type)               Output Shape         Param #  \n",
        "================================================================  \n",
        "            Conv2d-1            [-1, 6, 28, 28]             456  \n",
        "       BatchNorm2d-2            [-1, 6, 28, 28]              12  \n",
        "              ReLU-3            [-1, 6, 28, 28]               0  \n",
        "         MaxPool2d-4            [-1, 6, 14, 14]               0  \n",
        "            Conv2d-5           [-1, 16, 12, 12]             880  \n",
        "              ReLU-6           [-1, 16, 12, 12]               0  \n",
        "         MaxPool2d-7             [-1, 16, 6, 6]               0  \n",
        "            Conv2d-8             [-1, 32, 4, 4]           4,640  \n",
        "       BatchNorm2d-9             [-1, 32, 4, 4]              64  \n",
        "             ReLU-10             [-1, 32, 4, 4]               0  \n",
        "        MaxPool2d-11             [-1, 32, 2, 2]               0  \n",
        "           Conv2d-12             [-1, 64, 2, 2]           2,112  \n",
        "             ReLU-13             [-1, 64, 2, 2]               0  \n",
        "        MaxPool2d-14             [-1, 64, 1, 1]               0  \n",
        "           Linear-15                   [-1, 64]           4,160  \n",
        "             ReLU-16                   [-1, 64]               0  \n",
        "           Linear-17                   [-1, 10]             650  \n",
        "================================================================\n",
        "Total params: 12,974  \n",
        "Trainable params: 12,974  \n",
        "Non-trainable params: 0  \n",
        "----------------------------------------------------------------  \n",
        "Input size (MB): 0.01  \n",
        "Forward/backward pass size (MB): 0.17  \n",
        "Params size (MB): 0.05  \n",
        "Estimated Total Size (MB): 0.24  \n",
        "----------------------------------------------------------------  \n",
        "\n",
        "  It had 34.81% accuracy as well as 1.7583 test_loss.  \n",
        "  I have realized that this training can go further, because test_loss was decreasing and since training was very fast (I observed that problem later), I increased max_epoch to 200. And as a result I got 59% accuracy and ~1.17 test_loss:\n",
        "\n",
        "  During hyperparameter tuning, I realised that it uses 3 iterations per epoch and they I saw that batch size is set to be 20000 which probably resulted in out of memory error. Therefore I set it to be 128. And since I increased the batch size, it would be resulted in more training time, therefore I reduced max_epoch to 100 to avoid to much training and overfitting. Beside of all those, I tried stochastic gradient descent optimizer instead of Adam: \n",
        "     \n",
        "  ```\n",
        "optimizer = optim.SGD(my_classifier.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4) \n",
        "  ```  \n",
        "  After investigating more, I saw that in the data transformation, code uses wrong Normalization Values (it was very suspicious that normalization values are 0.5 in that mid-size dataset). Instead I searched on google and found correct values:\n",
        "\n",
        "```\n",
        "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "```\n",
        "\n",
        " \n",
        "[Github code for calculating those values](https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py)  \n",
        "  And after all these tunings, I ended up with 64% accuracy and 1.05 test_loss.\n",
        "\n",
        "---\n",
        "<font color=\"blue\"> \n",
        "VGG model\n",
        "---\n",
        "</font>\n",
        "\n",
        "**Model Summary**\n",
        "----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
        "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
        "              ReLU-3           [-1, 64, 32, 32]               0\n",
        "            Conv2d-4           [-1, 64, 32, 32]          36,928\n",
        "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
        "              ReLU-6           [-1, 64, 32, 32]               0\n",
        "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
        "            Conv2d-8          [-1, 128, 16, 16]          73,856\n",
        "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
        "             ReLU-10          [-1, 128, 16, 16]               0\n",
        "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
        "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
        "             ReLU-13          [-1, 128, 16, 16]               0\n",
        "        MaxPool2d-14            [-1, 128, 8, 8]               0\n",
        "           Conv2d-15            [-1, 256, 8, 8]         295,168\n",
        "      BatchNorm2d-16            [-1, 256, 8, 8]             512\n",
        "             ReLU-17            [-1, 256, 8, 8]               0\n",
        "           Conv2d-18            [-1, 256, 8, 8]         590,080\n",
        "      BatchNorm2d-19            [-1, 256, 8, 8]             512\n",
        "             ReLU-20            [-1, 256, 8, 8]               0\n",
        "           Conv2d-21            [-1, 256, 8, 8]         590,080\n",
        "      BatchNorm2d-22            [-1, 256, 8, 8]             512\n",
        "             ReLU-23            [-1, 256, 8, 8]               0\n",
        "           Conv2d-24            [-1, 256, 8, 8]         590,080\n",
        "      BatchNorm2d-25            [-1, 256, 8, 8]             512\n",
        "             ReLU-26            [-1, 256, 8, 8]               0\n",
        "        MaxPool2d-27            [-1, 256, 4, 4]               0\n",
        "           Conv2d-28            [-1, 512, 4, 4]       1,180,160\n",
        "      BatchNorm2d-29            [-1, 512, 4, 4]           1,024\n",
        "             ReLU-30            [-1, 512, 4, 4]               0\n",
        "           Conv2d-31            [-1, 512, 4, 4]       2,359,808\n",
        "      BatchNorm2d-32            [-1, 512, 4, 4]           1,024\n",
        "             ReLU-33            [-1, 512, 4, 4]               0\n",
        "           Conv2d-34            [-1, 512, 4, 4]       2,359,808\n",
        "      BatchNorm2d-35            [-1, 512, 4, 4]           1,024\n",
        "             ReLU-36            [-1, 512, 4, 4]               0\n",
        "           Conv2d-37            [-1, 512, 4, 4]       2,359,808\n",
        "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
        "             ReLU-39            [-1, 512, 4, 4]               0\n",
        "        MaxPool2d-40            [-1, 512, 2, 2]               0\n",
        "           Conv2d-41            [-1, 512, 2, 2]       2,359,808\n",
        "      BatchNorm2d-42            [-1, 512, 2, 2]           1,024\n",
        "             ReLU-43            [-1, 512, 2, 2]               0\n",
        "           Conv2d-44            [-1, 512, 2, 2]       2,359,808\n",
        "      BatchNorm2d-45            [-1, 512, 2, 2]           1,024\n",
        "             ReLU-46            [-1, 512, 2, 2]               0\n",
        "           Conv2d-47            [-1, 512, 2, 2]       2,359,808\n",
        "      BatchNorm2d-48            [-1, 512, 2, 2]           1,024\n",
        "             ReLU-49            [-1, 512, 2, 2]               0\n",
        "           Conv2d-50            [-1, 512, 2, 2]       2,359,808\n",
        "      BatchNorm2d-51            [-1, 512, 2, 2]           1,024\n",
        "             ReLU-52            [-1, 512, 2, 2]               0\n",
        "        MaxPool2d-53            [-1, 512, 1, 1]               0\n",
        "        AvgPool2d-54            [-1, 512, 1, 1]               0\n",
        "           Linear-55                   [-1, 10]           5,130\n",
        "================================================================\n",
        "Total params: 20,040,522\n",
        "Trainable params: 20,040,522  \n",
        "Non-trainable params: 0\n",
        "\n",
        "Input size (MB): 0.01  \n",
        "Forward/backward pass size (MB): 7.18  \n",
        "Params size (MB): 76.45  \n",
        "Estimated Total Size (MB): 83.64  \n",
        "\n",
        "---\n",
        "\n",
        "I looked up for the possible models and since our image size is 3x32x32, I decided to use VGGNet model, because it was easy to implement and it had much more parameters and layers and this  model could fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3×3) convolution filters in all layers. Due to the huge number of parameters, we can train this model more and avoid overfitting.\n",
        "\n",
        "Because of I was using more complex model, I changed the hyperparameters and add some code to train it faster. All of the tunings and changes I have made (except implementation of VGG)\n",
        "\n",
        "\n",
        "*   Despite of there is no significant difference in the performance between F.cross_entropy and nn.CrossEntropyLoss, I tried to use 2nd one. I succeeded to use in training process, however I could not achieve to use it on testing due to some errors. [Link](https://discuss.pytorch.org/t/f-cross-entropy-vs-torch-nn-cross-entropy-loss/25505)  \n",
        "*   max_epoch = 450 (In the beginning, I trained model without lr_scheduler and noticed that after around 100 epochs, training and test loss does not improve anymore, therefore I decided to use lr_scheduler and therefore I increased the number of epochs to see the affect of scheduler)\n",
        "learning_rate = 0.1 (In the beginning I wanted faster start and since I was using optimizer, it would not be problem)\n",
        "batch_size = 128 (I did not have time for more than 128, because of 12 hours limit)\n",
        "*  \n",
        "```\n",
        "if device == 'cuda':\n",
        "my_classifier = torch.nn.DataParallel(my_classifier)\n",
        "cudnn.benchmark = True\n",
        "```\n",
        "I added this line, because I felt that training is going very slow and after searching on google, I found [this](https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936)  and since input size was same, I decided to use and it made training slightly faster\n",
        "*\n",
        "```\n",
        "from torch.optim.lr_scheduler import MultiStepLR \n",
        "scheduler = MultiStepLR(optimizer, milestones=[150,300], gamma=0.1)\n",
        "```\n",
        "This is probably the most important thing that affected to results. This code allows me to manually set learning rate during the training process without stopping the training and loading model again with new learning rate.\n",
        "\n",
        "In the middle of training, I forgot to click on the page and I disconnected from server. First 160 epochs were trained until diconnection, then I loaded the model from checkpoint and trained rest of it.\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/file/d/1r--xWpkPpuwt2jatWb3ZkzbZ4PUbLYDB/view)\n",
        "\n",
        "After training for more than 8 hours, I got 89.76% accuracy and test loss was ~0.53. \n",
        "From both of the graphs, it obvious that how learning rate scheduler improved our training:\n",
        "\n",
        "[epoch:147, iteration:57500] train loss : 0.1708 train accuracy : 0.9219\n",
        "[epoch:147, iteration:57750] train loss : 0.3390 train accuracy : 0.9141\n",
        "[epoch:147, iteration:57868] test_loss : 0.6994 test accuracy : 0.7953\n",
        "[epoch:148, iteration:58000] train loss : 0.2231 train accuracy : 0.9297\n",
        "[epoch:148, iteration:58250] train loss : 0.2138 train accuracy : 0.9297\n",
        "[epoch:148, iteration:58259] test_loss : 0.7224 test accuracy : 0.7870\n",
        "[epoch:149, iteration:58500] train loss : 0.0212 train accuracy : 1.0000\n",
        "[epoch:149, iteration:58650] test_loss : 0.3882 test accuracy : 0.8910\n",
        "checkpoint is saved !\n",
        "[epoch:150, iteration:58750] train loss : 0.0240 train accuracy : 0.9922\n",
        "[epoch:150, iteration:59000] train loss : 0.0061 train accuracy : 1.0000\n",
        "[epoch:150, iteration:59041] test_loss : 0.4165 test accuracy : 0.8931\n",
        "checkpoint is saved !\n",
        "[epoch:151, iteration:59250] train loss : 0.0023 train accuracy : 1.0000\n",
        "[epoch:151, iteration:59432] test_loss : 0.4436 test accuracy : 0.8934\n",
        "checkpoint is saved !\n",
        "\n",
        "However, 2nd scheduling improved the training loss, it did not increase the accuracy and test loss did not decrease much.\n",
        "\n",
        "Using SGD instead of Adam is my personal choice. Maybe Adam is better, but since I did not have time and because of 12 hours limit, I have to try it later on my own.\n",
        "\n",
        "In order to improve the accuracy, we can use some data augmentation. I found transforms.RandomHorizontalFlip() (flips the picture) and transforms.RandomCrop() (crops some part of picture) functions that can be used in our train dataset for both improving accuracy and loss and giving ability to our model to predict flipped or shifted images. \n",
        "\n",
        "As we have seen that learning rate matters a lot, using Adam or Nesterov optimizer can also improve accuracy.\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XT5--AGcmNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
